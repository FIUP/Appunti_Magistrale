% !TEX encoding = UTF-8
% !TEX program = pdflatex
% !TEX root = InformationRetrieval.tex
% !TEX spellcheck = it-IT

% 28 Ottobre 2016
%\section{Modello probabilistico}
%\subsection{Basi di probabilità}

%\noindent Per trovare il valore di $p$ che più si avvicina alla sequenza di osservazioni è necessario effettuare la derivata della formula precedente e porla uguale a 0. Questo calcolo è complesso e quindi si preferisce effettuare la derivata del logaritmo.

Ad esempio se la sequenza di osservazioni del lancio di una moneta è $T,T, C$, ho che se provo a tracciare un grafico della funzione in base al variare di $p$ ho un massimo con $p = 2/3$, che coincide con la probabilità che ho osservato di ottenere testa.

\begin{align*}
	\frac{\partial}{\partial p} \log \Big( \prod\limits_{j = 1}^n p^{x^j}(1-p)^{1-x^j} \Big) &= 0\\
	\frac{\partial}{\partial p} \sum\limits_{j = 1}^n \log \Big( p^{x^j}(1-p)^{1-x^j} \Big) &= 0 \\
	\frac{\partial}{\partial p} \sum\limits_{j = 1}^n \log \Big( p^{x^j}\Big) + \sum\limits_{j = 1}^n \log \Big((1-p)^{1-x^j} \Big) &= 0 \\
	\frac{\partial}{\partial p} \sum\limits_{j = 1}^n x^j \log \Big( p\Big) + \sum\limits_{j = 1}^n (1-x^j)\log \Big(1-p \Big) &= 0 \\
	\sum\limits_{j = 1}^n x^j + \sum\limits_{j = 1}^n (1-x^j) \frac{1}{1-p}(-1)&= 0 \\
	n_{tR} \frac{1}{p} + (N_R - N_{tR})\frac{-1}{1-p} &= 0
	(1-p)n_{tR} &= (N_r - n_{tR}) p  = N_Rp - n_{tR}p\\
	p &= \frac{n_{tR}}{N_R}
\end{align*}

\noindent dove $n_{tR}$ è il numero di volte che ho osservato il termine rilevante e $N_R$ è il numero di termini rilevanti.
Per quanto riguarda i termini non rilevanti ho:

$$
q = \frac{n_{tNR}}{N_{NR}}
$$

\noindent Si ha quindi che 

$$
N = N_R + N_{NR} \qquad \text{e} \qquad n_t = n_{tR} + n_{tNR}
$$

\noindent Tutto questo vale però se abbiamo solo una moneta (o parola).

$$
P(C = rel | D=(T_1, T_2, T_3)) = \frac{P(D=(T_1,T_2, T_3) | C=rel)P(C=rel)}{P(D=(T_1, T_2, T_3))}
$$

\noindent In questo caso l'esperimento consiste nel lancio in contemporanea di 3 monete.
La differenza principale è che per avere una stima decente della probabilità devo osservare minimo 8 esiti, perché altrimenti alcune combinazioni non potrebbero mai uscire.
Questo è un problema, perché il numero minimo di esiti cresce in modo esponenziale rispetto al numero di termini/parole (\textbf{curse dimensionality}).

Per gestire questo problema viene fatta un'assunzione d'indipendenza: la cosi detta \textbf{Naive Bayes Indipendence Assumption}.

Grazie a questa assunzione posso semplificare il calcolo in:

$$
P(C = rel | D=(T_1, T_2, T_3)) = \prod\limits_{i = 1}^{|V|}P(T_i|C=rel)
$$

\noindent dove $V$ è il vocabolario dei termini, $V = \{ T_1, T_2, \ldots T_m\}$.

Nel nostro caso abbiamo anche che

$$
P(T_i|C=rel) \approx \text{Bernoulli}(p_i)
$$

\noindent perché possiamo dare ad ogni termine un certo peso.
Il problema adesso è quale peso dare ai vari termini.

Assumiamo che ci siano 4 documenti che sappiamo essere rilevanti e che contengono rispettivamente i termini $(t_1, t_3), (t_2), (t_1), (t_3)$. Abbiamo quindi:

\begin{align*}
P((x_11, x_21,\ldots x_m1),(x_12, x_22,\ldots x_m2)\ldots (x_1j, x_2j,\ldots x_mj) | C=rel) &= \prod\limits_{j=1}^n P(x_{1}^j,x_{2}^j,\ldots x_{m}^j|C=rel) \\
&= \prod\limits_{j=1}^n \prod\limits_{i=1}^m P(x_i | C=rel)
\end{align*}


\noindent Per calcolare le singole probabilità possiamo utilizzare le formule precedentemente viste:

$$
p_i = \frac{n_{t_iR}}{NR_i} \qquad \text{e} \qquad q_i = \frac{n_{t_iNR}}{N_{NR_i}}
$$

\noindent Quindi, per sapere se un documento $D$ che contiene i termini $t_1$ e $t_3$ ma non $t_2$ è rilevante, possiamo utilizzare:

$$
P(C=rel | D=\{t_1, \overline{t_2}, t_3\}) =\frac{ P( D=\{t_1, \overline{t_2}, t_3\} |C = rel)P(C=rel)}{P( D=\{t_1, \overline{t_2}, t_3\})}
$$

\noindent con le probabilità che possono o essere stimate o calcolate con le formule viste precedentemente.

Ma quando è che classifichiamo un documento come non rilevante? Quando

\begin{align*}
P(C=rel|D) &> P(C=\overline{rel} |D) \\
\frac{P(D | C=rel)P(C=rel)}{P(D)} &> \frac{P(D | C=\overline{rel})P(C=\overline{rel})}{P(D)} \\
P(D | C=rel)P(C=rel) &> P(D | C=\overline{rel})P(C=\overline{rel}) \\
\prod\limits_{i=1}^n p_{i}^x (1 - p_{i})^{(1-x)} P(C = rel) &> \prod\limits_{i=1}^n q_{i}^x (1 - q_{i})^{(1-x)} \\
\prod\limits_{i=1}^n \bigg(\frac{ p_{i} }{1-p_i}\bigg)^x (1-p) P(rel) &>\prod\limits_{i=1}^n \bigg(\frac{ q_{i} }{1-q_i}\bigg)^x (1-p) P(\overline{rel})
\end{align*}

\noindent Se la disequazione è rispettata, allora il documento può essere classificato come rilevante.

C'è però un problema, perché nel caso pratico ci sono vocabolari da 100000 termini e quindi le produttorie risultano essere incalcolabili, perché i numeri diventano troppo piccoli.
Per raggiare questo problema è necessario prendere il logaritmo delle produttorie, in modo da poter riscriverle come sommatorie:

\begin{align*}
\sum\limits_{i=1}^{n} \log\big( p_{i}^x (1 - p_{i})^{(1-x)} \big) P(rel) &> \sum\limits_{i=1}^n q_{i}^x (1 - q_{i})^{(1-x)} \\
\sum\limits_{i=1}^{n} \log\bigg( \frac{ p_{i} }{1-p_i} \bigg)^x +\sum\limits_{i=1}^{n} \log(1-p_i) + \log(P(rel)) &> \ldots \\
\sum\limits_{i=1}^{n} x \log\bigg( \frac{ p_{i} }{1-p_i} \bigg) +\sum\limits_{i=1}^{n} \log(1-p_i) + \log(P(rel)) &> \sum\limits_{i=1}^{n} x \log\bigg( \frac{ q_{i} }{1-q_i} \bigg) +\underbrace{\sum\limits_{i=1}^{n} \log(1-q_i) + \log(P(\overline{rel}))}_{\text{non dipendo dal documento}}
\end{align*}

\noindent Con questa nuova formulazione si ha che alcuni pezzi devono essere calcolati solo una volta ed inoltre se il termine $x_i$ non compare nel documento ($x_i=0$), non lo considero nella sommatoria e quindi nel lato pratico le sommatorie non vengono calcolate su tutto il dizionario. \todo{Capire se è del documento o della query. Tutte le x della formula sono $x_i$}

Posso inoltre riformulare ancora:

\begin{align*}
\sum\limits_{i=1}^n x_i \log\bigg( \frac{p_i}{1-p_i} \bigg) - \sum\limits_{i=1}^n x_i \log\bigg( \frac{q_i}{1-q_i} \bigg) &>\ldots \\
\sum\limits_{i=1}^n x_i \log\bigg( \frac{p_i}{1-p_i} \frac{1-q_i}{q_i} \bigg) &> \ldots \\
\ldots \text{\textit{math magic}}& \ldots \\
\sum\limits_{i=1}^n x_i \log\bigg( \frac{n_{t_R}}{N_R - n_{t_R}} \frac{N_{NR} - n_{t_{NR}}}{n_{t_{NR}}} \bigg) &> \ldots
\end{align*}

\noindent C'è ancora un problema se un termine non compare mai in un documento e quindi vengono aggiunte delle costanti $+0.5$, sia ai numeratori che ai denominatori.

$$
\underbrace{\log\bigg( \frac{n_{t_R} + 0.5}{N_R - n_{t_R} + 0.5} \frac{N_{NR} - n_{t_{NR}}+ 0.5}{n_{t_{NR}}+ 0.5} \bigg)}_{\textbf{Relevance Weight}}
$$

\noindent Così facendo quando non ho dati nel motore di ricerca o classificatore, ovvero $N_R = n_{t_R} =0 $, ottengo una giustificazione probabilistica per \textbf{IDF}.

\begin{align*}
\log\big(P(t_i)\big) &= \log\big(\frac{n_t}{N}\big) \text{\quad// document frequency}\\
	&= - \log\big( \frac{N}{n_t}\quad\text{inverse document frequency} \big) \\ 
	&\approx \log\bigg( \frac{n_{t_R} + 0.5}{N_R - n_{t_R} + 0.5} \frac{N_{NR} - n_{t_{NR}}+ 0.5}{n_{t_{NR}}+ 0.5} \bigg) 
\end{align*}


\noindent Questo modello probabilistico prende il nome di \textbf{Binary ... Model}.













